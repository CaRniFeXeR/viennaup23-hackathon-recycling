{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import OwlViTForObjectDetection, OwlViTProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from extract_features import get_features\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "semi = 2\n",
    "\n",
    "# NOTE: 1 and 2 OVERFIT massively! Only use with a LOT of data!\n",
    "\n",
    "# 0 = fully unsupervised (features and bboxes from the model)\n",
    "# 1 = semi-supervised (handlabeled data and features from the model)\n",
    "# 2 = fully supervised (handlabeled data and external feature extractor)\n",
    "\n",
    "if semi == 0:\n",
    "    classes = [[\"object\", \"Can: Sealed cylindrical metal container\", \"Bottle: Narrow-necked liquid storage container\"]]\n",
    "\n",
    "    features = get_features(texts=classes)\n",
    "\n",
    "    features_torch = []\n",
    "    class_ids_torch = []\n",
    "    patches = []\n",
    "\n",
    "    # extract features and make tensors\n",
    "    for feature in features:\n",
    "        feature_tensor = feature[0].detach().cpu()\n",
    "        label = feature[1]\n",
    "        class_id = feature[2]\n",
    "\n",
    "        features_torch.append(feature_tensor)\n",
    "        class_ids_torch.append(torch.as_tensor(class_id))\n",
    "        patches.append(feature[3])\n",
    "\n",
    "    # stack lists:\n",
    "    X_data = torch.stack(features_torch)\n",
    "    y_data = torch.stack(class_ids_torch)\n",
    "\n",
    "elif semi == 1:\n",
    "    root_dir = '/caa/Homes01/mburges/viennaup23-hackathon-recycling/backend/data/'\n",
    "    data_json = json.load(open(root_dir + 'labels.json'))\n",
    "    image_path = os.path.join(root_dir, 'images')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "        model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "        base_model = model.owlvit\n",
    "\n",
    "        del(model)\n",
    "\n",
    "        base_model.eval()\n",
    "        base_model.to(\"cuda\")\n",
    "\n",
    "        features_torch = []\n",
    "        class_ids_torch = []\n",
    "        for file in tqdm(data_json):\n",
    "            image = Image.open(os.path.join(image_path, file))\n",
    "\n",
    "            inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "            image_features = base_model.get_image_features(**inputs).squeeze()\n",
    "\n",
    "            features_torch.append(image_features.detach().cpu())\n",
    "            class_ids_torch.append(torch.as_tensor(data_json[file]))\n",
    "\n",
    "        X_data = torch.stack(features_torch)\n",
    "        y_data = torch.stack(class_ids_torch)\n",
    "\n",
    "else:\n",
    "    root_dir = '/caa/Homes01/mburges/viennaup23-hackathon-recycling/backend/data/'\n",
    "    data_json = json.load(open(root_dir + 'labels.json'))\n",
    "\n",
    "    features_path = os.path.join(root_dir, 'features_noclip64')\n",
    "\n",
    "    features_torch = []\n",
    "    class_ids_torch = []\n",
    "    for file in data_json:\n",
    "        npy_ = file.replace('.png', '.npy')\n",
    "        features = np.load(os.path.join(features_path, npy_))\n",
    "\n",
    "        features_torch.append(torch.as_tensor(features))\n",
    "        class_ids_torch.append(torch.as_tensor(data_json[file]))\n",
    "    \n",
    "    # stack lists:\n",
    "    X_data = torch.stack(features_torch).float()\n",
    "    y_data = torch.stack(class_ids_torch)\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloKo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FloKo, self).__init__()\n",
    "        self.fc = nn.Linear(64, 32)\n",
    "        self.dropout_1 = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "        self.fc4 = nn.Linear(2, torch.stack(class_ids_torch).max()+1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.relu(x)\n",
    "        intermediate = self.fc3(x)\n",
    "        x = self.fc4(intermediate)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x, intermediate\n",
    "\n",
    "model = FloKo()\n",
    "\n",
    "print(summary(model, (64,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 250\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# split data into train and test\n",
    "X_train = X_data[:int(len(X_data)*0.8)]\n",
    "y_train = y_data[:int(len(y_data)*0.8)]\n",
    "\n",
    "X_test = X_data[int(len(X_data)*0.8):]\n",
    "y_test = y_data[int(len(y_data)*0.8):]\n",
    "\n",
    "# patches_train = patches[:int(len(patches)*0.8)]\n",
    "# patches_test = patches[int(len(patches)*0.8):]\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "# Create a TensorDataset\n",
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "# Create a DataLoader\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "losses_train, losses_test = [], []\n",
    "for epoch in trange(num_epochs):\n",
    "    total_loss_train = []\n",
    "    total_loss_test = []\n",
    "    for inputs, targets in dataloader:\n",
    "        model.train()\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        targets = targets.to(\"cuda\")\n",
    "\n",
    "        # encode targets hot one\n",
    "        targets = torch.nn.functional.one_hot(targets, num_classes=3).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, intermediate = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(output, target=targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        total_loss_train.append(loss.item())\n",
    "    for inputs, targets in test_loader:\n",
    "        model.eval()\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        targets = targets.to(\"cuda\")\n",
    "\n",
    "        # encode targets hot one\n",
    "        targets = torch.nn.functional.one_hot(targets, num_classes=3).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, intermediate = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(output, target=targets)\n",
    "        total_loss_test.append(loss.item())\n",
    "    losses_train.append(np.mean(total_loss_train))\n",
    "    losses_test.append(np.mean(total_loss_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(losses_train, label='train')\n",
    "plt.plot(losses_test, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model over dataset and get features\n",
    "model.eval()\n",
    "\n",
    "features = []\n",
    "class_ids = []\n",
    "predictions = []\n",
    "\n",
    "for inputs, labels in tqdm(test_loader):\n",
    "    inputs = inputs.cuda()\n",
    "    outputs, intermediate = model(inputs)\n",
    "    features.append(intermediate.squeeze().detach().cpu().numpy())\n",
    "    predictions.append(outputs.squeeze().detach().cpu().numpy())\n",
    "    class_ids.append(labels.detach().cpu().numpy())\n",
    "\n",
    "features = np.array(features)\n",
    "class_ids = np.array(class_ids)\n",
    "predictions = np.argmax(np.array(predictions), axis=1)\n",
    "\n",
    "print(features.shape)\n",
    "print(class_ids.shape)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights of last layer\n",
    "weights = model.fc4.weight.detach().cpu().numpy()\n",
    "\n",
    "# plot features and labels in matplotlib\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title('Labels')\n",
    "ax1.scatter(features[:, 0], features[:, 1], c=class_ids, cmap='coolwarm')\n",
    "ax2.set_title('Predictions')\n",
    "ax2.scatter(features[:, 0], features[:, 1], c=predictions, cmap='coolwarm')\n",
    "#draw lines of weights\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vvv IGNORE vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(class_ids.shape)\n",
    "print(predictions.shape)\n",
    "print(len(patches_test))\n",
    "\n",
    "save_path = \"/caa/Homes01/mburges/viennaup23-hackathon-recycling/backend/data/images_and_json_for_UI\"\n",
    "\n",
    "for i in range(len(patches_test)):\n",
    "    patch = patches_test[i]\n",
    "    patch.save(os.path.join(save_path, f\"patch_{i}.png\"))\n",
    "\n",
    "    # save json\n",
    "    json_ = {\"features\": features[i].tolist(), \"class_id\": class_ids[i].tolist(), \"name\": f\"patch_{i}.png\"}\n",
    "    with open(os.path.join(save_path, f\"pred_before_patch_{i}.json\"), 'w') as f:\n",
    "        json.dump(json_, f)\n",
    "\n",
    "    # save json\n",
    "    json_ = {\"features\": features[i].tolist(), \"class_id\": predictions[i].tolist(), \"name\": f\"patch_{i}.png\"}\n",
    "    with open(os.path.join(save_path, f\"pred_after_patch_{i}.json\"), 'w') as f:\n",
    "        json.dump(json_, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SegAny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
